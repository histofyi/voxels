








# imports used in the notebook

import json
import os
import requests
import time
import hashlib

from collections import Counter

import matplotlib.pyplot as plt
import numpy as np


plt.rcParams['axes.facecolor'] = 'white'

from hdbscan import HDBSCAN

from functions.helpers import load_config, deslugify_allele, get_max_count, percentage, tensorize
from functions.structures import fetch_structure_info, download_structure, load_pdb_file_to_dataframe, calculate_max_rmsd_for_cluster
from functions.voxels import create_voxel_grid, find_voxels_for_structure, neighbour_distance



# load the config file to populate the configuration variables dictionary
# Note: if you change the shape and parameters of the voxel map, you will need to update the config file with the new voxel_map_hash

config = load_config()

# output the config file to the console
config





sql_query = "select pdb_code, locus, allele_slug, peptide_sequence, resolution from core where peptide_length='9' and peptide_features='correct_sequence_and_length' and complex_type='class_i_with_peptide' and (locus='hla-a' or locus='hla-b' or locus='hla-c') order by resolution asc"

# the only_highest_resolution flag is set to False, so all structures will be returned, not just the highest resolution ones
structure_info = fetch_structure_info(sql_query, config['excluded_structures'], only_highest_resolution=False)

print (structure_info['metadata'])

print (f"There are {len(structure_info['structures'])} structures in the dataset.")

# TODO add a check to make sure that we're not removing structures with different conformations. 
# TODO add in getting more structures to test (e.g. if there are _1, _2, etc. structures, get those too)
# TODO add in some way of checking alternate atom locations (e.g. A, B, C, etc.)

if structure_info['metadata']['only_highest_resolution']:
    print (f"{structure_info['metadata']['original_structure_count'] - structure_info['metadata']['only_highest_resolution_count']} lower resolution duplicate structures have been removed.")

pdb_code_list = list(structure_info['structures'].keys())








loci = {}
alleles = {}

for pdb_code in pdb_code_list:
  locus = structure_info['structures'][pdb_code]['locus']
  allele_slug = structure_info['structures'][pdb_code]['allele_slug']

  if locus not in loci:
    loci[locus] = 0
  loci[locus] += 1

  if allele_slug not in alleles:
    alleles[allele_slug] = 0
  alleles[allele_slug] += 1



# Create plots of number of structures per locus

fig, ax = plt.subplots()
ax.pie(loci.values(), labels=[locus.upper() for locus in loci.keys()])
ax.set_title('Structures by loci')
plt.tight_layout()

print (f"Number of structures containing HLA-A*02:01: {alleles['hla_a_02_01']}\n\n")

# Create plots of number of structures per allele

fig, ax = plt.subplots()
ax.pie(alleles.values(), labels=[deslugify_allele(allele_slug) for allele_slug in alleles.keys()])
ax.set_title('Structures by allele')
plt.tight_layout()










peptide_dict = {}

for pdb_code in pdb_code_list:
  peptide = structure_info['structures'][pdb_code]['peptide']
  if peptide not in peptide_dict:
    peptide_dict[peptide] = []
  peptide_dict[peptide].append(pdb_code)

sequences = list(peptide_dict.keys())

print (f"There are {len(sequences)} unique peptide sequences in the dataset.")

iedb_url = "https://api-nextgen-tools.iedb.org/api/v1/pipeline"

iedb_payload = {
  "pipeline_id": "",
  "run_stage_range": [
    1,
    1
  ],
  "stages": [
    {
      "name": "Cluster",
      "stage_number": 1,
      "tool_group": "cluster",
      "input_sequence_text": '\n'.join(sequences),
      "input_parameters": {
        "cluster_pct_identity": 0.5,
        "peptide_length_range": [
          9,
          9
        ],
        "predictors": [
          {
            "type": "cluster",
            "method": "cliques"
          }
        ]
      }
    }
  ]
}

iedb_response = requests.post(iedb_url, json=iedb_payload)

iedb_response_json = iedb_response.json()

iedb_results_uri = iedb_response_json['results_uri']

print (f"IEDB clustering job '{iedb_response_json['result_id']}' started.")

iedb_results_json = {'status': 'running'}

while iedb_results_json['status'] != 'done':
  iedb_results = requests.get(iedb_results_uri)
  iedb_results_json = iedb_results.json()

  print (f"IEDB clustering job '{iedb_results_json['id']}' is {iedb_results_json['status']}")

  if iedb_results_json['status'] in ['started', 'pending']:
    time.sleep(5)





table_data = iedb_results.json()['data']['results'][0]['table_data']

raw_clusters = {}

cluster_sizes = {}

peptide_count = 0

for row in table_data:
    cluster_id = row[0]
    if not cluster_id in raw_clusters:
        raw_clusters[cluster_id] = {'consensus': None, 'members': [], 'size': 0}
    if not 'Consensus' in row[1]:
        if not row[2] in raw_clusters[cluster_id]['members']:
            raw_clusters[cluster_id]['members'].append(row[2])
            raw_clusters[cluster_id]['size'] += 1
            peptide_count += 1
    else:
        raw_clusters[cluster_id]['consensus'] = row[2]
    

print (peptide_count)

for cluster in raw_clusters:
    cluster_size = raw_clusters[cluster]['size']
    if not cluster_size in cluster_sizes:
        cluster_sizes[cluster_size] = {'count': 0, 'canonicals': [], 'total': 0}
    cluster_sizes[cluster_size]['count'] += 1
    if raw_clusters[cluster]['consensus']:
        cluster_sizes[cluster_size]['canonicals'].append(raw_clusters[cluster]['consensus'])
        
    else:
        cluster_sizes[cluster_size]['canonicals'].append(raw_clusters[cluster]['members'][0])
    cluster_sizes[cluster_size]['total'] += cluster_size


labels = [f"{item} peptides" for item in list(cluster_sizes.keys())]
sizes = [cluster_sizes[label]['total'] for label in list(cluster_sizes.keys())]

labels, sizes

fig, ax = plt.subplots()
ax.pie(sizes, labels=labels)
ax.set_title('Peptide sequence clusters by size')
plt.tight_layout()






# create the voxel grid, we only need to do this once, so it's outside the loop
voxel_grid = create_voxel_grid(config['centre_of_mass'], config['box_xyz'], config['voxel_size'], range_offset=1, y_offset=8)

voxel_grid_hash = hashlib.md5(str(voxel_grid).encode()).hexdigest()

voxel_set_filepath = f"output/voxel_sets/{voxel_grid_hash}"

if not os.path.exists(voxel_set_filepath):
    os.mkdir(voxel_set_filepath)
    print (f"Created directory {voxel_set_filepath}\n")

with open(f"{voxel_set_filepath}/voxel_set.json", 'w') as filehandle:
    json.dump(voxel_grid, filehandle, indent=4)
    print (f"Voxel set saved to {voxel_set_filepath}/voxel_set.json\n")

print (f"Voxel set hash for this grid is: {voxel_grid_hash}")







structure_count = 0
download_count = 0
cached_count = 0
errors = []

structure_dataset = {}

for pdb_code in pdb_code_list:
    structure_data, downloaded, cached, error  = download_structure(pdb_code, config)
    if downloaded:
        download_count += 1
    if cached:
        cached_count += 1
    if error:
        print (error)
        errors.append(pdb_code)
    else:
        structure_dataset[pdb_code] = structure_data
    structure_count += 1

print (f"{structure_count} structures were processed. {download_count} structures were downloaded, {cached_count} structures were already downloaded, and {len(errors)} errors occurred.")






voxelized_structures = []

structure_voxel_collection = {}
used_voxel_collection = {}
position_voxel_collection = {}
occupied_voxel_list = []

# iterate through the test PDB codes
for pdb_code in pdb_code_list:

    # load the peptide dataframe for the PDB code
    dataframe = load_pdb_file_to_dataframe(pdb_code, 'peptide', config)
    
    if dataframe:
    # if the dataframe is not None, find the voxels for the structure and write them to a file
        structure_voxels = find_voxels_for_structure(dataframe, voxel_grid['voxels'], voxel_size=config['voxel_size'])

        voxel_data = {
            'voxel_grid_hash': voxel_grid_hash,
            'structure_voxels': structure_voxels
        }

        filename = f"{voxel_set_filepath}/{pdb_code}.json"

        with open(filename, 'w') as filehandle:
            json.dump(voxel_data, filehandle, indent=4)
        voxelized_structures.append(pdb_code)

        structure_voxel_collection[pdb_code] = structure_voxels

        used_voxels = [structure_voxels[position]['voxel_label'] for position in structure_voxels]
        used_voxel_collection[pdb_code] = used_voxels

        position = 1
        for voxel_label in used_voxels:
            if position not in position_voxel_collection:
                position_voxel_collection[position] = {}
            if voxel_label not in position_voxel_collection[position]:
                position_voxel_collection[position][voxel_label] = {'count':0,'members':[]}
            if pdb_code not in position_voxel_collection[position][voxel_label]['members']:
                position_voxel_collection[position][voxel_label]['members'].append(pdb_code)
                position_voxel_collection[position][voxel_label]['count'] += 1
            if voxel_label not in occupied_voxel_list:
                occupied_voxel_list.append(voxel_label)
            position += 1


print (f"{len(structure_voxel_collection)} structures were processed and saved to {voxel_set_filepath}")
print (f"There are {len(occupied_voxel_list)} different voxels used.")





all_used_voxels = []

for position in position_voxel_collection:

    position_voxel_names = [voxel_name for voxel_name in voxel_grid['voxels'].keys() if voxel_name in position_voxel_collection[position]]

    all_used_voxels += position_voxel_names

    used_voxel_count = [position_voxel_collection[position][voxel_name]['count']/structure_count * 100 for voxel_name in position_voxel_names]

    plt.figure(figsize=(20, 5))
    plt.bar(position_voxel_names, used_voxel_count, color='tab:blue')

    plt.title(f"Voxels used for Position {position}")
    plt.xlabel("Voxel name")
    plt.ylabel("Percentage of structures with occupancy in a specific voxel")
    plt.gca().yaxis.set_major_formatter(plt.FuncFormatter('{:.0f}%'.format))
    plt.ylim(0, 100)
    plt.xticks(rotation=90)

    plt.show()







position_mode_set = {}


labels = []
mode_percentages = []
neighbour_percentages = []

for position in position_voxel_collection:

  neighbour_count = 0
  labels.append(position)
  mode_voxel_info = get_max_count(position_voxel_collection[position])
  mode_voxel_name = mode_voxel_info['max_voxel_used']
  mode_voxel_count = mode_voxel_info['count']

  position_mode_set[position] = {
      'modal_voxel': {
          'voxel_name':mode_voxel_name,
          'count':mode_voxel_count
      },
      'neighbour_voxels':[],
      'stats':{}
  }

  mode_percentage = percentage(mode_voxel_count, structure_count)

  mode_percentages.append(mode_percentage)

  position_mode_set[position]['stats']['mode_percentage'] = mode_percentage

  mode_and_neighbour_count = get_max_count(position_voxel_collection[position])['count']
  for voxel in position_voxel_collection[position]:
    if voxel != mode_voxel_name:
      if neighbour_distance(voxel, mode_voxel_name) < 1.5:
        neighbour_voxel = {
            'voxel_name': voxel,
            'count': position_voxel_collection[position][voxel]['count']
        }
        neighbour_count += position_voxel_collection[position][voxel]['count']
      position_mode_set[position]['neighbour_voxels'].append(neighbour_voxel)
  mode_and_neighbour_count += neighbour_count
  mode_and_neighbour_percentage = percentage(mode_and_neighbour_count, structure_count )

  neighbour_percentage = percentage(neighbour_count, structure_count)
  neighbour_percentages.append(neighbour_percentage)
  position_mode_set[position]['stats']['neighbour_percentage'] = neighbour_percentage
  position_mode_set[position]['stats']['mode_and_neighbour_percentage'] = mode_and_neighbour_percentage


plt.bar(labels, mode_percentages, color='tab:blue')
plt.bar(labels, neighbour_percentages, bottom=mode_percentages, color='tab:cyan')
plt.xlabel("Peptide position")
plt.ylabel("Percentage of structures with occupancy in a modal or neighbouring voxel")
plt.gca().yaxis.set_major_formatter(plt.FuncFormatter('{:.0f}%'.format))
plt.xticks(labels)
plt.legend(['mode voxel','neighbouring voxel'])
plt.show()

for position in position_mode_set:
  print (f"\nPosition {position}")
  print (position_mode_set[position]['stats'])






possible_clustering_positions = [[4,5], [4,6], [4,7], [5,6] ,[5,7], [6,7], [4,5,6], [5,6,7], [4,5,6,7], [1,2,3,4,5,6,7,8,9]]

min_cluster_size = 3

clustering_collection = {}

used_voxel_tensors = [tensorize(voxel_labels) for voxel_labels in used_voxel_collection.values()]

structure_labels = list(used_voxel_collection.keys())

clustering_collection = {
    'clusters': {},
    'structure_labels':structure_labels,
    'structure_count':len(structure_labels)
}

clusters_path = f"output/clusters/{voxel_grid_hash}"

if not os.path.exists(clusters_path):
    os.mkdir(clusters_path)

cluster_sizes_path = f"{clusters_path}/cluster_size_{min_cluster_size}"

if not os.path.exists(cluster_sizes_path):
    os.mkdir(cluster_sizes_path)

for clustering_position_set in possible_clustering_positions:
    cluster_positions_str = '_'.join([str(position) for position in clustering_position_set])
    
    print (f"Clustering on {clustering_position_set} with minimum cluster size {min_cluster_size}")
    clustering_data = []

    for row in used_voxel_tensors:
        p = 1
        processed_row = []
        for position in row:
            if p in clustering_position_set:

                for item in position:
                    processed_row.append(item)
                
            p += 1
        clustering_data.append(processed_row)
    
    hdb = HDBSCAN(min_cluster_size=min_cluster_size, gen_min_span_tree=True)
    hdb.fit(clustering_data)

    cluster_sizes = Counter(hdb.labels_)
    cluster_sizes.pop(-1, None)

    clusters = {}
    noise_cluster = []

    for cluster in set(hdb.labels_):
        if cluster != -1:
            clusters[str(cluster + 1)] = [structure_labels[i] for i in range(len(hdb.labels_)) if hdb.labels_[i] == cluster]
        else:
            noise_cluster = [structure_labels[i] for i in range(len(hdb.labels_)) if hdb.labels_[i] == -1]

        all_clusters_rmsds = {}
        for cluster_number in clusters:
            cluster_members = clusters[cluster_number]

            cluster_max_rmsd = calculate_max_rmsd_for_cluster(cluster_members, structure_voxel_collection)

            all_clusters_rmsds[cluster_number] = cluster_max_rmsd

      

    noise_cluster_max_rmsd = calculate_max_rmsd_for_cluster(noise_cluster, structure_voxel_collection)

    all_clusters_rmsds['noise'] = noise_cluster_max_rmsd

    print (f"Number of clusters: {len(clusters)} | Number of structures in the noise cluster: {len(noise_cluster)}")

    cluster_max_rmsds = [all_clusters_rmsds[cluster] for cluster in all_clusters_rmsds if cluster != 'noise']

    print (cluster_max_rmsds)

    print (f"Max RMSD {max(cluster_max_rmsds)}")    

    average_cluster_all_position_rmsd = round(sum([all_clusters_rmsds[cluster] for cluster in all_clusters_rmsds]) / len(all_clusters_rmsds), 2)
    
    print (f"Avg max RMSD for all positions: {average_cluster_all_position_rmsd}")

    print (f"Noise cluster for all positions max RMSD: {noise_cluster_max_rmsd}")
    
    print ('')


    clustering_collection['clusters'][cluster_positions_str] = {
        'clustering_positions':clustering_position_set,
        'cluster_count':len(set(hdb.labels_)) -1,
        'noise_cluster_size': list(hdb.labels_).count(-1),
        'min_cluster_size': min(cluster_sizes.values()),
        'max_cluster_size': max(cluster_sizes.values()),
        'mean_cluster_size':round(sum(cluster_sizes.values()) / len(cluster_sizes),1),
        'max_rmsd':average_cluster_all_position_rmsd,
        'cluster_max_rmsds':cluster_max_rmsds,
        'noise_cluster_max_rmsd':noise_cluster_max_rmsd,
        'noise_cluster':noise_cluster,
        'outlier_scores':[],
        'clusters':clusters
    }

    with open(f"{cluster_sizes_path}/clustering_{cluster_positions_str}.json", 'w') as filehandle:
        json.dump(clustering_collection['clusters'][cluster_positions_str], filehandle, indent=4)




#print (clustering_collection)





scores = []
labels = [item.replace('_',', ') for item in clustering_collection['clusters'].keys()]

for clustering in clustering_collection['clusters']:
    noise_cluster_size = clustering_collection['clusters'][clustering]['noise_cluster_size']
    cluster_count = clustering_collection['clusters'][clustering]['cluster_count']
    max_rmsd = clustering_collection['clusters'][clustering]['max_rmsd']
    max_noise_rmsd = clustering_collection['clusters'][clustering]['noise_cluster_max_rmsd']

    print (clustering)


    score = round(noise_cluster_size * max_rmsd, 2)
    scores.append(score)
    print (f"Score: {score} | Noise cluster size: {noise_cluster_size} | Cluster count: {cluster_count} | Max RMSD: {max_rmsd} | Max noise RMSD: {max_noise_rmsd}")

print (f"\nMinimum score is {min(scores)}")


plt.bar(labels, scores, color='tab:blue')
plt.title(f"Score for each clustering position set (minimum cluster size {min_cluster_size})")
plt.xlabel("Clustering positions used")
plt.ylabel("Score")
plt.xticks(rotation=90)
plt.show()


cluster_counts = [clustering_collection['clusters'][clustering]['cluster_count'] for clustering in clustering_collection['clusters']]


plt.bar(labels, cluster_counts, color='tab:blue')
plt.title(f"Number of clusters found for each clustering position set (minimum cluster size {min_cluster_size})")
plt.xlabel("Clustering positions used")
plt.ylabel("Number of clusters found")
plt.xticks(rotation=90)
plt.show()

noise_cluster_sizes = [clustering_collection['clusters'][clustering]['noise_cluster_size'] for clustering in clustering_collection['clusters']]

plt.bar(labels, noise_cluster_sizes, color='tab:blue')
plt.title(f"Size of the noise cluster for each clustering position set (minimum cluster size {min_cluster_size})")
plt.xlabel("Clustering positions used")
plt.ylabel("Number of structures in the noise cluster")
plt.xticks(rotation=90)
plt.show()

max_rmsds = [clustering_collection['clusters'][clustering]['max_rmsd'] for clustering in clustering_collection['clusters']]

plt.bar(labels, max_rmsds, color='tab:blue')
plt.title(f"Maximum positional RMSD for each clustering position set (minimum cluster size {min_cluster_size})")
plt.xlabel("Clustering positions used")
plt.ylabel("Maximum RMSD for a cluster")
plt.gca().yaxis.set_major_formatter(plt.FuncFormatter('{:.1f}Å'.format))
plt.xticks(rotation=90)
plt.show()

noise_cluster_max_rmsds = [clustering_collection['clusters'][clustering]['noise_cluster_max_rmsd'] for clustering in clustering_collection['clusters']]

plt.bar(labels, noise_cluster_max_rmsds, color='tab:blue')
plt.title(f"Maximum noise cluster positional RMSD for each clustering position set (minimum cluster size {min_cluster_size})")
plt.xlabel("Clustering positions used")
plt.ylabel("Maximum RMSD for the noise cluster")
plt.gca().yaxis.set_major_formatter(plt.FuncFormatter('{:.1f}Å'.format))
plt.xticks(rotation=90)
plt.show()








clustering_positions = [4,5,6]
print (f"Minimum cluster size {min_cluster_size} and clustering positions {clustering_positions}")

clustering_positions_string = '_'.join([str(position) for position in clustering_positions])


all_clusters_used_voxels = []
for cluster_number in clustering_collection['clusters'][clustering_positions_string]['clusters']:
    cluster = clustering_collection['clusters'][clustering_positions_string]['clusters'][cluster_number]
    cluster_size = len(cluster)
    cluster_voxels = {}

    for member in cluster:
        for voxel in used_voxel_collection[member]:
            if voxel not in cluster_voxels:
                cluster_voxels[voxel] = 0
            cluster_voxels[voxel] += 1

    cluster_used_voxels = []
    for voxel in all_used_voxels:
        if voxel not in cluster_voxels:
            cluster_used_voxels.append(np.nan)
        else:
            cluster_used_voxels.append(round(cluster_voxels[voxel] / cluster_size * 100, 2))
    all_clusters_used_voxels.append(cluster_used_voxels)

y_labels = [f"Cluster {cluster_number} ({len(clustering_collection['clusters'][clustering_positions_string]['clusters'][cluster_number])} members)" for cluster_number in clustering_collection['clusters'][clustering_positions_string]['clusters']]
x_labels = all_used_voxels


import seaborn as sns

fig, ax = plt.subplots(figsize=(40,20))
cmap = sns.cm.rocket_r
hm = sns.heatmap(data = all_clusters_used_voxels, cmap=cmap, ax=ax, xticklabels=x_labels, yticklabels=y_labels) 
plt.show()





# TODO work on this section, bring over from alphafold analysis notebook







# TODO work on this section 





# TODO work on this section






